#!/usr/bin/env bash
set -Eeuo pipefail

# vllm-turbo: Qwen3-Next-80B-A3B NVFP4 + MTP Speculative Decoding
# Image: vllm-turbo (Avarok dgx-vllm v22)
# Expected: ~67 tok/s avg, peak 111.9 tok/s on DGX Spark

NAME="vllm-turbo-mtp"
MODEL="${MODEL:-/data/tensordata/qwen3-next-80B-Thinking-NVFP4}"

echo "Starting ${NAME}..."
echo "Model: ${MODEL}"
echo "MTP: 2 speculative tokens"
echo "Port: 8011 (mapped from 8888)"

podman run -d \
  --replace \
  --name "${NAME}" \
  --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  --hooks-dir=/usr/share/containers/oci/hooks.d \
  --ipc=host \
  -p 8011:8888 \
  -v /data/tensordata:/data/tensordata \
  -v "${HOME}/.cache/huggingface:/root/.cache/huggingface" \
  -e VLLM_USE_FLASHINFER_MOE_FP4=0 \
  -e VLLM_TEST_FORCE_FP8_MARLIN=1 \
  -e VLLM_NVFP4_GEMM_BACKEND=marlin \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  -e MODEL="${MODEL}" \
  -e PORT=8888 \
  -e GPU_MEMORY_UTIL=0.90 \
  -e MAX_MODEL_LEN=65536 \
  -e MAX_NUM_SEQS=128 \
  -e "VLLM_EXTRA_ARGS=--attention-backend flashinfer --kv-cache-dtype fp8 --speculative-config {\"method\":\"mtp\",\"num_speculative_tokens\":2} --no-enable-chunked-prefill" \
  vllm-turbo:latest serve

echo "Container ${NAME} started. Logs: podman logs -f ${NAME}"
echo "Health: curl http://localhost:8011/health"
echo "Note: MTP requires ~10 min startup (model load + torch.compile + CUDA graphs)"
