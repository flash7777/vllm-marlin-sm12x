#!/bin/bash
# EP=2 Qwen3-Coder-30B-A3B INT4 AutoRound: vLLM Serve
# Expert Parallelism: TP=2 + enable-expert-parallel
# 128 Experts auf 2 GPUs verteilt (64 pro Node)
#
# Reihenfolge:
#   1. DGX:  ./start.qwen3coder_ep2.head
#   2. PGX:  ./start.qwen3coder_ep2.worker
#   3. DGX:  ./start.qwen3coder_ep2.serve  <-- DU BIST HIER

set -euo pipefail

CONTAINER_NAME="qwen3coder-head"
MODEL_PATH="/data/tensordata/Qwen3-Coder-30B-A3B-Instruct-int4-AutoRound"
PORT=8011
LOGFILE="/tmp/qwen3coder-ep2-serve.log"

echo "=== Ray Cluster Status pruefen ==="
podman exec "$CONTAINER_NAME" ray status
echo ""

echo "=== vLLM serve starten (EP=2) ==="
echo "Modell:  $MODEL_PATH"
echo "Port:    $PORT"
echo "TP=2 + EP: 128 Experts / 2 = 64 pro GPU"
echo ""

nohup podman exec "$CONTAINER_NAME" \
  vllm serve "$MODEL_PATH" \
    --host 0.0.0.0 \
    --port "$PORT" \
    --served-model-name qwen3-coder \
    --tensor-parallel-size 2 \
    --enable-expert-parallel \
    --distributed-executor-backend ray \
    --gpu-memory-utilization 0.05 \
    --kv-cache-memory-bytes 10G \
    --max-model-len 16384 \
    --trust-remote-code \
    --quantization auto_round \
    --disable-log-requests \
  > "$LOGFILE" 2>&1 &

SERVE_PID=$!
echo "vLLM serve gestartet (PID: $SERVE_PID)"
echo "Logs: tail -f $LOGFILE"
echo ""
echo "Warte auf Modell-Laden..."
echo "Health-Check: curl http://localhost:$PORT/health"
